# 03_build_html.py  (feeds.txt + meta description scrape + å¼·åŒ–ãƒ•ã‚£ãƒ«ã‚¿)
import re
import html
import time
import socket
import urllib.request
from datetime import datetime, timezone
from urllib.parse import urlparse
from pathlib import Path

import feedparser
from bs4 import BeautifulSoup

# --- paths ---
ROOT = Path(r"C:\iwate_news")
CONFIG_DIR = ROOT / "config"
SITE_DIR = ROOT / "site"
SITE_DIR.mkdir(parents=True, exist_ok=True)

SITE_TITLE = "å²©æ‰‹çœŒ ä¸å‹•ç”£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚ï¼ˆè¦‹å‡ºã—ï¼‹ãƒªãƒ³ã‚¯ï¼‰"
SITE_DESC  = "å²©æ‰‹Ã—ä¸å‹•ç”£ãƒ»åœŸåœ°ãƒ»å»ºè¨­ãƒ»éƒ½å¸‚è¨ˆç”»ã®æ–°ç€æƒ…å ±ã‚’RSSã‹ã‚‰è‡ªå‹•æŠ½å‡ºï¼ˆè¦ç´„ãªã—ãƒ»è»½é‡MVPï¼‰"
MAX_ITEMS = 300

# --- timeouts & scraping options ---
socket.setdefaulttimeout(6)  # network default timeout (seconds)
USE_PAGE_SCRAPE = True       # ãƒšãƒ¼ã‚¸æœ¬æ–‡ã¯è¦‹ã«è¡Œã‹ãªã„ã€‚meta description ã®ã¿
MAX_SCRAPE_PER_RUN = 10      # 1å›ã®å®Ÿè¡Œã§å®Ÿãƒšãƒ¼ã‚¸ã‚’è¦‹ã«è¡Œãä¸Šé™
SCRAPE_TIMEOUT = 5           # metaå–å¾—ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

# --- keywords (èª¿æ•´ã¯ã“ã“ã§ã€‚å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã¯å…¥ã‚Œãªã„ã“ã¨) ---
# ãƒˆãƒ”ãƒƒã‚¯ç³»ï¼ˆä¸å‹•ç”£ãƒ»éƒ½å¸‚è¨ˆç”»ãªã©ï¼‰
TOPIC_KEYWORDS = [
    "ä¸å‹•ç”£","åœ°ä¾¡","åœ°ä¾¡èª¿æŸ»","å…¬ç¤ºåœ°ä¾¡","è·¯ç·šä¾¡","å›ºå®šè³‡ç”£ç¨","åœ°ä¾¡æŒ‡æ•°",
    "ä½å®…","ç©ºãå®¶","ç©ºå®¶","è³ƒè²¸","åˆ†è­²","ãƒãƒ³ã‚·ãƒ§ãƒ³","æˆ¸å»º","å›£åœ°",
    "ç”¨åœ°","ç”¨åœ°å–å¾—","åç”¨","ä¿ç•™åœ°","é€ æˆ","å®…åœ°","å®…åœ°é€ æˆ","åŒºç”»","åŒºç”»æ•´ç†",
    "éƒ½å¸‚è¨ˆç”»","ç”¨é€”åœ°åŸŸ","å¸‚è¡—åŒ–","åœ°åŒºè¨ˆç”»","ç«‹åœ°é©æ­£åŒ–","å†é–‹ç™º","å†æ•´å‚™",
    "PFI","PPP","åœŸåœ°","å»ºç‰©","è€æœ½åŒ–","å»ºã¦æ›¿ãˆ","å»ºæ›¿","ç‰©ä»¶","ç€å·¥"
    "ç«£å·¥","è§£ä½“","é–‹æ¥­","é–‰æ¥­","é–‹åº—","é–‰åº—","ç”¨é€”å¤‰æ›´","å£²å´","è­²æ¸¡","åˆ©æ´»ç”¨",
    "åº—èˆ—","å·¥å ´","è¦³å…‰","ãƒ›ãƒ†ãƒ«","çµŒæ¸ˆåŠ¹æœ","çµ±è¨ˆ","æ¨ç§»","åœŸåœ°","å»ºç‰©"
]
# åœ°åï¼ˆå²©æ‰‹ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
GEO_KEYWORDS = [
    "å²©æ‰‹","ç››å²¡","èŠ±å·»","åŒ—ä¸Š","å¥¥å·","ä¸€é–¢","äºŒæˆ¸","ä¹…æ…ˆ","å®®å¤","å¤§èˆ¹æ¸¡","é™¸å‰é«˜ç”°","æ»æ²¢",
    "é é‡","è‘›å·»","å²©æ³‰","å±±ç”°","æ™®ä»£","é‡ç”°","æ´‹é‡","ä½ç”°","å…«å¹¡å¹³","ç´«æ³¢","çŸ¢å·¾","é‡‘ã‚±å´","é‡‘ãƒ¶å´","é›«çŸ³","å¤§æ§Œ","ç”°é‡ç•‘","è»½ç±³","å²©æ‰‹ç”º","ä¹æˆ¸","è¥¿å’Œè³€","å¹³æ³‰"
]
# é™¤å¤–èªï¼ˆã‚¹ãƒãƒ¼ãƒ„ãƒ»å¤©æ°—ãƒ»ç´”ã‚¨ãƒ³ã‚¿ç­‰ã€‚å¿…è¦ã«å¿œã˜ã¦å¢—æ¸›ï¼‰
NEGATIVE_KEYWORDS = [
    "é«˜æ ¡é‡çƒ","ãƒ—ãƒ­é‡çƒ","Jãƒªãƒ¼ã‚°","ã‚µãƒƒã‚«ãƒ¼","ãƒ©ã‚°ãƒ“ãƒ¼","ãƒãƒ¬ãƒ¼ãƒœãƒ¼ãƒ«","ãƒã‚¹ã‚±ãƒƒãƒˆãƒœãƒ¼ãƒ«","ãƒ†ãƒ‹ã‚¹","ã‚´ãƒ«ãƒ•",
    "ã‚³ãƒ³ã‚µãƒ¼ãƒˆ","ãƒ©ã‚¤ãƒ–","éŸ³æ¥½","èˆå°","ã‚¤ãƒ™ãƒ³ãƒˆæƒ…å ±",
    "å¤©æ°—","æ°—æ¸©","çŒ›æš‘","å¯’æ³¢","é™é›ª","å°é¢¨","åœ°éœ‡é€Ÿå ±"
]

# --- feeds fallback (feeds.txt ãŒç„¡ã„/ç©ºã®ã¨ã) ---
DEFAULT_FEEDS = [
    "https://www.pref.iwate.jp/news.rss",
    "https://www.city.morioka.iwate.jp/news.rss",
]

def read_feeds_from_txt(path: Path) -> list[str]:
    if not path.exists():
        return []
    text = None
    for enc in ("utf-8-sig", "cp932"):
        try:
            text = path.read_text(encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    if text is None:
        text = path.read_text(encoding="utf-8", errors="ignore")

    urls, seen = [], set()
    for line in text.splitlines():
        s = line.strip()
        if not s or s.startswith("#"):
            continue
        if not (s.lower().startswith("http://") or s.lower().startswith("https://")):
            continue
        if s not in seen:
            seen.add(s)
            urls.append(s)
    return urls

def clean_html(s: str) -> str:
    s = html.unescape(s or "")
    return re.sub(r"<[^>]+>", "", s)

def host_of(url: str) -> str:
    try:
        return urlparse(url).netloc
    except Exception:
        return ""

def to_iso(dt) -> str:
    if dt:
        return datetime(*dt[:6], tzinfo=timezone.utc).isoformat()
    return datetime.now(timezone.utc).isoformat()

def iso_to_ymd_jst(iso: str) -> str:
    try:
        dt = datetime.fromisoformat(iso.replace("Z", "+00:00")).astimezone()
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return ""

def count_hits(text: str, words: list[str]) -> int:
    if not text:
        return 0
    return sum(1 for w in words if w in text)

def is_iwate_gov_domain(netloc: str) -> bool:
    # è‡ªæ²»ä½“ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„å²©æ‰‹é–¢é€£ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯åœ°åã‚¹ã‚³ã‚¢ä»£æ›¿ã¨ã—ã¦æ‰±ã†
    nl = (netloc or "").lower()
    return nl.endswith(".iwate.jp") or nl.endswith(".lg.jp") or ("iwate" in nl)

def filter_match(text: str, netloc: str) -> bool:
    # é™¤å¤–èªãŒå«ã¾ã‚Œã‚‹ãªã‚‰å³NG
    if any(w in (text or "") for w in NEGATIVE_KEYWORDS):
        return False
    topic = count_hits(text, TOPIC_KEYWORDS)
    geo   = count_hits(text, GEO_KEYWORDS)
    # å—ç†ãƒ«ãƒ¼ãƒ«ï¼š
    # 1) ãƒˆãƒ”ãƒƒã‚¯1ä»¥ä¸Š ã‹ã¤ (åœ°å1ä»¥ä¸Š or å²©æ‰‹ç³»ãƒ‰ãƒ¡ã‚¤ãƒ³)
    if topic >= 1 and (geo >= 1 or is_iwate_gov_domain(netloc)):
        return True
    # 2) ãƒˆãƒ”ãƒƒã‚¯ãŒ2èªä»¥ä¸Šãƒ’ãƒƒãƒˆï¼ˆåœ°åãªã—ã§ã‚‚é€šã™ï¼‰â€¦æ‹¾ã„æ¼ã‚Œå¯¾ç­–
    if topic >= 2:
        return True
    return False

def _fetch_meta_description(url: str, timeout: int = SCRAPE_TIMEOUT) -> str:
    try:
        req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
        with urllib.request.urlopen(req, timeout=timeout) as resp:
            html_bytes = resp.read()
        soup = BeautifulSoup(html_bytes, "html.parser")
        tag = soup.find("meta", property="og:description") or soup.find("meta", attrs={"name": "description"})
        return (tag.get("content") or "").strip() if tag else ""
    except Exception:
        return ""

def fetch_items(feeds: list[str]):
    items = []
    scraped = 0
    total_entries = 0

    for feed_url in feeds:
        print(f"[fetch] {feed_url}")
        start = time.time()
        try:
            d = feedparser.parse(feed_url)
        except Exception as e:
            print(f"[error] {feed_url} -> {e}")
            continue
        took = time.time() - start
        print(f"[ok] {feed_url} entries={len(d.entries)} {took:.1f}s")

        for e in d.entries:
            total_entries += 1
            title = (e.get("title") or "").strip()
            link  = e.get("link") or ""
            netloc = host_of(link)

            # RSSå†…ã®æœ¬æ–‡å€™è£œ
            body = ""
            if e.get("content") and isinstance(e["content"], list) and e["content"]:
                body = clean_html(e["content"][0].get("value") or "")
            if not body:
                body = clean_html(e.get("summary") or e.get("description") or "")

            # æœ¬æ–‡ãŒè–„ã„æ™‚ã ã‘ meta description ã‚’è¦‹ã‚‹ï¼ˆä¸Šé™ã¤ãï¼‰
            if USE_PAGE_SCRAPE and not body and link and scraped < MAX_SCRAPE_PER_RUN:
                meta = _fetch_meta_description(link, timeout=SCRAPE_TIMEOUT)
                if meta:
                    body = meta
                    scraped += 1
                    print(f"[scrape] {link} -> meta description captured")

            haystack = f"{title}\n{body}"

            if filter_match(haystack, netloc):
                # æ—¥ä»˜
                pub = None
                for key in ("published_parsed", "updated_parsed"):
                    if e.get(key):
                        pub = to_iso(e.get(key))
                        break
                if not pub:
                    pub = to_iso(None)

                items.append({
                    "title": title,
                    "url": link,
                    "source": netloc,
                    "published": pub,
                })

    items.sort(key=lambda x: x["published"], reverse=True)
    print(f"[sum] total_entries={total_entries}, extracted={len(items)}, scraped={scraped}")
    return items[:MAX_ITEMS]

def build_html(items):
    css = """
    body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,"Noto Sans JP",sans-serif;line-height:1.6;margin:20px;}
    header{margin-bottom:16px}
    h1{font-size:1.45rem;margin:0}
    .desc{color:#555;margin:4px 0 12px}
    .date{margin-top:22px;font-weight:700}
    .item{border:1px solid #eee;border-radius:12px;padding:10px 12px;margin:10px 0}
    .item h3{margin:0 0 6px;font-size:1.02rem}
    .meta{font-size:.85rem;color:#666}
    a{color:#0a58ca;text-decoration:none}
    a:hover{text-decoration:underline}
    footer{color:#777;font-size:.85rem;margin-top:24px}
    """
    groups = {}
    for it in items:
        day = iso_to_ymd_jst(it["published"]) or "æ—¥ä»˜ä¸æ˜"
        groups.setdefault(day, []).append(it)

    parts = [
        "<!DOCTYPE html>",
        "<html lang=\"ja\">",
        "<meta charset=\"utf-8\">",
        f"<title>{html.escape(SITE_TITLE)}</title>",
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">",
        f"<meta name=\"description\" content=\"{html.escape(SITE_DESC)}\">",
        f"<style>{css}</style>",
        "<body>",
        "<header>",
        f"<h1>{html.escape(SITE_TITLE)}</h1>",
        f"<div class=\"desc\">{html.escape(SITE_DESC)}</div>",
        "</header>",
    ]
    for day in sorted(groups.keys(), reverse=True):
        parts.append(f"<div class=\"date\">ğŸ“… {day}</div>")
        for it in groups[day]:
            title = html.escape(it["title"] or "(ç„¡é¡Œ)")
            url   = html.escape(it["url"] or "#")
            src   = html.escape(it["source"] or "")
            parts.append(
                f"<div class=\"item\">"
                f"<h3><a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{title}</a></h3>"
                f"<div class=\"meta\">å‡ºå…¸: {src}</div>"
                f"</div>"
            )
    parts += [
        "<footer>è‡ªå‹•ç”Ÿæˆï¼ˆè¦‹å‡ºã—ï¼‹ãƒªãƒ³ã‚¯ã®ã¿, feeds.txt è¨­å®š / meta description å‚ç…§ï¼‰/ Î²ç‰ˆ</footer>",
        "</body></html>",
    ]
    out = SITE_DIR / "index.html"
    out.write_text("\n".join(parts), encoding="utf-8")
    return out

def main():
    feeds = read_feeds_from_txt(CONFIG_DIR / "feeds.txt")
    if not feeds:
        feeds = DEFAULT_FEEDS
        print(f"[info] feeds.txt ãŒè¦‹ã¤ã‹ã‚‰ãªã„/ç©ºã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆFEEDS({len(feeds)})ã‚’ä½¿ç”¨")
    else:
        print(f"[info] feeds.txt ã‹ã‚‰ {len(feeds)} æœ¬ã®RSSã‚’èª­ã¿è¾¼ã¿")
    items = fetch_items(feeds)
    out = build_html(items)
    print(f"ç”Ÿæˆ: {out}ï¼ˆ{len(items)}ä»¶ï¼‰")

if __name__ == "__main__":
    main()
