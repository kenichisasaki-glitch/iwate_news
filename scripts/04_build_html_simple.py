# 04_build_html_simple.py â€” è¶…ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆå…¨æ–‡è²¼ã‚Šæ›¿ãˆç”¨ï¼‰
# ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ï¼‹RSSæœ¬æ–‡(ã‚ã‚Œã°)ã®ã¿ã§åˆ¤å®šï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã—ï¼‰
# ãƒ»feeds.txt ã§ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã€Œè¿½åŠ (+) / ä¸Šæ›¸ã(=) / ALLï¼ˆå…¨ä»¶ï¼‰ã€å¯¾å¿œ
# ãƒ»ALL ã§ã‚‚ 3åˆ—ç›®ã®é™¤å¤–èªã¯æœ‰åŠ¹
# ãƒ»æ—¥æœ¬èªã®è¡¨è¨˜ã‚†ã‚Œã«å¼·ãã™ã‚‹ãŸã‚ NFKC æ­£è¦åŒ–

import os
import re
import html
import time
import socket
import unicodedata
from datetime import datetime, timezone
from urllib.parse import urlparse
from pathlib import Path

import feedparser

# ==== ãƒ‘ã‚¹ãƒ»åŸºæœ¬è¨­å®š ====
ROOT = Path(os.getenv("IWATE_ROOT", ".")).resolve()
CONFIG_DIR = ROOT / "config"
SITE_DIR = ROOT / "site"
SITE_DIR.mkdir(parents=True, exist_ok=True)

SITE_TITLE = "å²©æ‰‹çœŒ ä¸å‹•ç”£ã¾ã¨ã‚ã‚µã‚¤ãƒˆ<br>ï¼ˆæ¯æ—¥7:00è‡ªå‹•æ›´æ–°ï¼‰"
SITE_DESC  = '<a href="https://www.greo-jp.com/" target="_blank">GREOåˆåŒä¼šç¤¾ãŒé‹å–¶ã™ã‚‹ã¾ã¨ã‚ã‚µã‚¤ãƒˆã§ã™ã€‚</a>'
MAX_ITEMS = 300

socket.setdefaulttimeout(6)  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã®å®‰å…¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

# ==== ã‚°ãƒ­ãƒ¼ãƒãƒ«èªï¼ˆãƒ™ãƒ¼ã‚¹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šï¼‰====
GLOBAL_INCLUDE = [
    "ä¸å‹•ç”£","åœ°ä¾¡","åœ°ä¾¡èª¿æŸ»","å…¬ç¤ºåœ°ä¾¡","è·¯ç·šä¾¡","å›ºå®šè³‡ç”£ç¨","åœ°ä¾¡æŒ‡æ•°",
    "ä½å®…","ç©ºãå®¶","ç©ºå®¶","è³ƒè²¸","åˆ†è­²","ãƒãƒ³ã‚·ãƒ§ãƒ³","æˆ¸å»º","å›£åœ°",
    "ç”¨åœ°","ç”¨åœ°å–å¾—","åç”¨","ä¿ç•™åœ°","é€ æˆ","å®…åœ°","å®…åœ°é€ æˆ","åŒºç”»","åŒºç”»æ•´ç†",
    "éƒ½å¸‚è¨ˆç”»","ç”¨é€”åœ°åŸŸ","å¸‚è¡—åŒ–","åœ°åŒºè¨ˆç”»","ç«‹åœ°é©æ­£åŒ–","å†é–‹ç™º","å†æ•´å‚™",
    "PFI","PPP","åœŸåœ°","å»ºç‰©","è€æœ½åŒ–","å»ºã¦æ›¿ãˆ","å»ºæ›¿","ç‰©ä»¶","ç€å·¥","é–‰é¤¨",
    "ç«£å·¥","è§£ä½“","é–‹æ¥­","é–‰æ¥­","é–‹åº—","é–‰åº—","ç”¨é€”å¤‰æ›´","å£²å´","è­²æ¸¡","åˆ©æ´»ç”¨",
    "åº—èˆ—","å·¥å ´","è¦³å…‰","ãƒ›ãƒ†ãƒ«","çµŒæ¸ˆåŠ¹æœ","çµ±è¨ˆ","æ¨ç§»","åœŸåœ°","å»ºç‰©",
    "å»ºç¯‰","é–‰æ ¡","å»ƒæ ¡","çµ±å»ƒåˆ","è·¡åœ°","ãƒ”ã‚«ãƒ‡ãƒªãƒ¼"
]

GLOBAL_EXCLUDE = [
    # ãƒã‚¤ã‚ºã«ãªã‚ŠãŒã¡ãªè©±é¡Œ
    "æš´é¢¨","é›·","å°é¢¨","è¢«å®³","ç«ç½","å…¨ç„¼","åŠç„¼","ç„¼ã‘è·¡","ã‚¯ãƒ","ã‚°ãƒ",
    "çŒ›æš‘","å¤©å€™","å¤©æ°—"
]

# ==== ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆFEEDSï¼ˆfeeds.txtãŒç©º/ç„¡ã„ã¨ãï¼‰====
DEFAULT_FEEDS = [
    "https://www.pref.iwate.jp/news.rss",
    "https://www.city.morioka.iwate.jp/news.rss",
]

# ==== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
def clean_html(s: str) -> str:
    s = html.unescape(s or "")
    return re.sub(r"<[^>]+>", "", s)

def host_of(url: str) -> str:
    try:
        return urlparse(url).netloc
    except Exception:
        return ""

def to_iso(dt) -> str:
    if dt:
        return datetime(*dt[:6], tzinfo=timezone.utc).isoformat()
    return datetime.now(timezone.utc).isoformat()

def iso_to_ymd_jst(iso: str) -> str:
    try:
        dt = datetime.fromisoformat(iso.replace("Z", "+00:00")).astimezone()
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return ""

def norm(s: str) -> str:
    # å…¨è§’/åŠè§’ãƒ»æ¿ç‚¹ãªã©ã‚’çµ±ä¸€ã—ã¦ã‹ã‚‰å°æ–‡å­—åŒ–
    s = unicodedata.normalize("NFKC", s or "")
    return s.lower()

def any_hit(text_lc: str, words: list[str]) -> bool:
    if not words:
        return False
    for w in set(words):
        w = norm(w)
        if w and w in text_lc:
            return True
    return False

def none_hit(text_lc: str, words: list[str]) -> bool:
    if not words:
        return True
    for w in set(words):
        w = norm(w)
        if w and w in text_lc:
            return False
    return True

# ==== feeds.txt ã®èª­ã¿è¾¼ã¿ ====
# 1è¡Œ:  URL | <å«ã‚ã‚‹èªspec> | <é™¤å¤–èªspec>
# <spec>:
#   "= èª èª ..."  â†’ ä¸Šæ›¸ãï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡è¦–ï¼‰
#   "+ èª èª ..."  â†’ è¿½åŠ ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¶³ã™ï¼‰
#   "èª èª ..."    â†’ è¿½åŠ ï¼ˆæ¥é ­è¾ãªã—ã¯ + ã¨åŒã˜ï¼‰
#   "ALL" / "*" / "ALL!" â†’ ã“ã®RSSã¯å…¨ä»¶é€šã™ï¼ˆ2åˆ—ç›®ã«æ›¸ãï¼‰ã€‚ãŸã ã—3åˆ—ç›®ã®é™¤å¤–èªã¯æœ‰åŠ¹
def read_feeds_with_rules(path: Path):
    if not path.exists():
        return []
    text = None
    for enc in ("utf-8-sig", "cp932"):
        try:
            text = path.read_text(encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    if text is None:
        text = path.read_text(encoding="utf-8", errors="ignore")

    def parse_spec(spec: str):
        spec = spec.strip()
        mode = "add"  # add or override
        if not spec:
            return mode, []
        head = spec[:1]
        if head == "=":
            mode = "override"; spec = spec[1:].strip()
        elif head in {"+", "-"}:
            mode = "add"; spec = spec[1:].strip()
        words = [w for w in spec.split() if w]
        return mode, words

    feeds = []
    for line in text.splitlines():
        s = line.strip()
        if not s or s.startswith("#"):
            continue
        parts = [p.strip() for p in s.split("|")]
        url = parts[0]
        inc_spec = parts[1] if len(parts) >= 2 else ""
        exc_spec = parts[2] if len(parts) >= 3 else ""

        inc_mode, inc_words = parse_spec(inc_spec)
        exc_mode, exc_words = parse_spec(exc_spec)

        inc_upper = inc_spec.strip().upper()
        pass_all = inc_upper in ("ALL", "*", "ALL!")

        # ALLã§ã‚‚ exc_words ã¯ä¿æŒï¼ˆinc ã¯ç„¡è¦–ï¼‰
        feeds.append({
            "url": url,
            "pass_all": pass_all,
            "inc_mode": "override" if pass_all else inc_mode,
            "inc_words": [] if pass_all else inc_words,
            "exc_mode": exc_mode,
            "exc_words": exc_words,
        })
    return feeds

# ==== ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º â†’ HTML ====
def fetch_items(feed_rules: list[dict]):
    items = []
    total_entries = 0
    seen_urls = set()

    if not feed_rules:
        feed_rules = [{"url": u, "pass_all": False,
                       "inc_mode":"add","inc_words":[],
                       "exc_mode":"add","exc_words":[]} for u in DEFAULT_FEEDS]
        print(f"[info] feeds.txt ãŒç„¡ã„/ç©º â†’ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ{len(feed_rules)}æœ¬ã§å®Ÿè¡Œ")

    for fr in feed_rules:
        url = fr["url"]
        pass_all = fr.get("pass_all", False)

        # inc/exc æ±ºå®šã€‚ALLæ™‚ã¯ inc ã‚’ä½¿ã‚ãšã€exc ã¯æ´»ã‹ã™
        if pass_all:
            inc = []
            exc = fr["exc_words"] if fr["exc_mode"] == "override" else (GLOBAL_EXCLUDE + fr["exc_words"])
        else:
            inc = fr["inc_words"] if fr["inc_mode"] == "override" else (GLOBAL_INCLUDE + fr["inc_words"])
            exc = fr["exc_words"] if fr["exc_mode"] == "override" else (GLOBAL_EXCLUDE + fr["exc_words"])

        print(f"[fetch] {url} {'(ALL)' if pass_all else ''}")
        start = time.time()
        try:
            d = feedparser.parse(url)
        except Exception as e:
            print(f"[error] {url} â†’ {e}")
            continue
        print(f"[ok] {url} entries={len(d.entries)} {time.time()-start:.1f}s")

        for e in d.entries:
            total_entries += 1
            title = (e.get("title") or "").strip()
            link  = e.get("link") or ""
            # RSSã®æœ¬æ–‡/è¦ç´„ï¼ˆã‚ã‚Œã°ï¼‰
            body = ""
            if e.get("content") and isinstance(e["content"], list) and e["content"]:
                body = clean_html(e["content"][0].get("value") or "")
            if not body:
                body = clean_html(e.get("summary") or e.get("description") or "")

            hay = f"{title}\n{body}"
            hay_lc = norm(hay)

            # å—ç†åˆ¤å®šï¼šALLãªã‚‰é™¤å¤–èªã ã‘ãƒã‚§ãƒƒã‚¯ï¼é€šå¸¸ã¯ includeâˆ§(not exclude)
            if pass_all:
                accept = none_hit(hay_lc, exc)
            else:
                accept = any_hit(hay_lc, inc) and none_hit(hay_lc, exc)

            if not accept:
                continue

            # URLé‡è¤‡ã‚’é™¤å¤–
            if link and link in seen_urls:
                continue
            if link:
                seen_urls.add(link)

            # æ—¥ä»˜
            pub = None
            for key in ("published_parsed", "updated_parsed"):
                if e.get(key):
                    pub = to_iso(e.get(key))
                    break
            if not pub:
                pub = to_iso(None)

            items.append({
                "title": title,
                "url": link,
                "source": host_of(link),
                "published": pub,
            })

    items.sort(key=lambda x: x["published"], reverse=True)
    print(f"[sum] total_entries={total_entries}, extracted={len(items)}")
    return items[:MAX_ITEMS]

def build_html(items):
    css = """
    body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,"Noto Sans JP",sans-serif;line-height:1.6;margin:20px;}
    header{margin-bottom:16px}
    h1{font-size:1.45rem;margin:0}
    .desc{color:#555;margin:4px 0 12px}
    .date{margin-top:22px;font-weight:700}
    .item{border:1px solid #eee;border-radius:12px;padding:10px 12px;margin:10px 0}
    .item h3{margin:0 0 6px;font-size:1.02rem}
    .meta{font-size:.85rem;color:#666}
    a{color:#0a58ca;text-decoration:none}
    a:hover{text-decoration:underline}
    footer{color:#777;font-size:.85rem;margin-top:24px}
    """
    groups = {}
    for it in items:
        day = iso_to_ymd_jst(it["published"]) or "æ—¥ä»˜ä¸æ˜"
        groups.setdefault(day, []).append(it)

    parts = [
    "<!DOCTYPE html>",
    "<html lang=\"ja\">",
    "<meta charset=\"utf-8\">",
    f"<title>{html.escape(SITE_TITLE)}</title>",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">",
    f"<meta name=\"description\" content=\"{html.escape(SITE_DESC)}\">",
    f"<style>{css}</style>",
    "<body>",
    "<header>",
    f"<h1>{SITE_TITLE}</h1>",
    f'<div class="desc">{SITE_DESC}</div>',   # â† escape ã‚’å¤–ã—ãŸ
    "</header>",
    ]
    for day in sorted(groups.keys(), reverse=True):
        parts.append(f"<div class=\"date\">ğŸ“… {day}</div>")
        for it in groups[day]:
            title = html.escape(it["title"] or "(ç„¡é¡Œ)")
            url   = html.escape(it["url"] or "#")
            src   = html.escape(it["source"] or "")
            parts.append(
                f"<div class=\"item\">"
                f"<h3><a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{title}</a></h3>"
                f"<div class=\"meta\">å‡ºå…¸: {src}</div>"
                f"</div>"
            )
    parts += [
        '<footer><a href="https://www.greo-jp.com/" target="_blank">Operated by GREO</a></footer>',
        "</body></html>",
    ]
    out = SITE_DIR / "index.html"
    out.write_text("\n".join(parts), encoding="utf-8")
    return out

def main():
    feeds_path = CONFIG_DIR / "feeds.txt"
    feed_rules = read_feeds_with_rules(feeds_path)
    items = fetch_items(feed_rules)
    out = build_html(items)
    print(f"ç”Ÿæˆ: {out}ï¼ˆ{len(items)}ä»¶ï¼‰")

if __name__ == "__main__":
    main()
