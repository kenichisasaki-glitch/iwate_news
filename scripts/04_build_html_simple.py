# 04_build_html_simple.py â€” è¶…ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆå…¨æ–‡è²¼ã‚Šæ›¿ãˆç”¨ï¼‰
# ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ï¼‹RSSæœ¬æ–‡(ã‚ã‚Œã°)ã®ã¿ã§åˆ¤å®šï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãªã—ï¼‰
# ãƒ»feeds.txt ã§ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥ã«ã€Œè¿½åŠ (+) / ä¸Šæ›¸ã(=) / ALLï¼ˆå…¨ä»¶ï¼‰ã€å¯¾å¿œ
# ãƒ»ALL ã§ã‚‚ 3åˆ—ç›®ã®é™¤å¤–èªã¯æœ‰åŠ¹
# ãƒ»æ—¥æœ¬èªã®è¡¨è¨˜ã‚†ã‚Œã«å¼·ãã™ã‚‹ãŸã‚ NFKC æ­£è¦åŒ–

import os
import re
import html
import time
import socket
import unicodedata
from datetime import datetime, timezone
from urllib.parse import urlparse
from pathlib import Path

import feedparser

# ==== ãƒ‘ã‚¹ãƒ»åŸºæœ¬è¨­å®š ====
ROOT = Path(os.getenv("IWATE_ROOT", ".")).resolve()
CONFIG_DIR = ROOT / "config"
SITE_DIR = ROOT / "site"
SITE_DIR.mkdir(parents=True, exist_ok=True)

SITE_TITLE_TEXT = "å²©æ‰‹çœŒ ä¸å‹•ç”£ã¾ã¨ã‚ã‚µã‚¤ãƒˆï¼ˆæ¯æ—¥7:00è‡ªå‹•æ›´æ–°ï¼‰"  # â† ã‚¿ãƒ–è¡¨ç¤ºç”¨ï¼ˆæ”¹è¡Œãªã—ï¼‰
SITE_TITLE_HTML = "å²©æ‰‹çœŒ ä¸å‹•ç”£ã¾ã¨ã‚ã‚µã‚¤ãƒˆ<br>ï¼ˆæ¯æ—¥7:00è‡ªå‹•æ›´æ–°ï¼‰"  # â† ãƒšãƒ¼ã‚¸è¦‹å‡ºã—ç”¨
SITE_DESC  = '<a href="https://www.greo-jp.com/" target="_blank">GREOåˆåŒä¼šç¤¾ãŒé‹å–¶ã™ã‚‹ã¾ã¨ã‚ã‚µã‚¤ãƒˆã§ã™ã€‚</a>'
MAX_ITEMS = 1000

socket.setdefaulttimeout(6)  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã®å®‰å…¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

# ==== ã‚°ãƒ­ãƒ¼ãƒãƒ«èªï¼ˆãƒ™ãƒ¼ã‚¹ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šï¼‰====
GLOBAL_INCLUDE = [
    "ä¸å‹•ç”£","åœ°ä¾¡","åœ°ä¾¡èª¿æŸ»","å…¬ç¤ºåœ°ä¾¡","è·¯ç·šä¾¡","å›ºå®šè³‡ç”£ç¨","åœ°ä¾¡æŒ‡æ•°",
    "ä½å®…","ç©ºãå®¶","ç©ºå®¶","è³ƒè²¸","åˆ†è­²","ãƒãƒ³ã‚·ãƒ§ãƒ³","æˆ¸å»º","å›£åœ°",
    "ç”¨åœ°","ç”¨åœ°å–å¾—","åç”¨","ä¿ç•™åœ°","é€ æˆ","å®…åœ°","å®…åœ°é€ æˆ","åŒºç”»","åŒºç”»æ•´ç†",
    "éƒ½å¸‚è¨ˆç”»","ç”¨é€”åœ°åŸŸ","å¸‚è¡—åŒ–","åœ°åŒºè¨ˆç”»","ç«‹åœ°é©æ­£åŒ–","å†é–‹ç™º","å†æ•´å‚™","å†ç”Ÿ",
    "PFI","PPP","åœŸåœ°","å»ºç‰©","è€æœ½åŒ–","å»ºã¦æ›¿ãˆ","å»ºæ›¿","ç‰©ä»¶","ç€å·¥","é–‰é¤¨",
    "ç«£å·¥","è§£ä½“","é–‹æ¥­","é–‰æ¥­","é–‹åº—","é–‰åº—","ç”¨é€”å¤‰æ›´","å£²å´","è­²æ¸¡","åˆ©æ´»ç”¨",
    "åº—èˆ—","å·¥å ´","è¦³å…‰","ãƒ›ãƒ†ãƒ«","çµŒæ¸ˆåŠ¹æœ","çµ±è¨ˆ","æ¨ç§»","æ–½è¨­","è€äººãƒ›ãƒ¼ãƒ ",
    "å»ºç¯‰","é–‰æ ¡","å»ƒæ ¡","çµ±å»ƒåˆ","è·¡åœ°","æ—…é¤¨","æ¸©æ³‰","æ”¹ä¿®","ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°ã‚»ãƒ³ã‚¿ãƒ¼",
]

GLOBAL_EXCLUDE = [
    # ãƒã‚¤ã‚ºã«ãªã‚ŠãŒã¡ãªè©±é¡Œ
    "æš´é¢¨","é›·","å°é¢¨","è¢«å®³","ç«ç½","å…¨ç„¼","åŠç„¼","ç„¼ã‘è·¡","ã‚¯ãƒ","ã‚°ãƒ",
    "çŒ›æš‘","å¤©å€™","å¤©æ°—"
]

# ==== ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆFEEDSï¼ˆfeeds.txtãŒç©º/ç„¡ã„ã¨ãï¼‰====
DEFAULT_FEEDS = [
    "https://www.pref.iwate.jp/news.rss",
    "https://www.city.morioka.iwate.jp/news.rss",
]

# ==== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====
def clean_html(s: str) -> str:
    s = html.unescape(s or "")
    return re.sub(r"<[^>]+>", "", s)

def host_of(url: str) -> str:
    try:
        return urlparse(url).netloc
    except Exception:
        return ""

def to_iso(dt) -> str:
    if dt:
        return datetime(*dt[:6], tzinfo=timezone.utc).isoformat()
    return datetime.now(timezone.utc).isoformat()

def iso_to_ymd_jst(iso: str) -> str:
    try:
        dt = datetime.fromisoformat(iso.replace("Z", "+00:00")).astimezone()
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return ""

def norm(s: str) -> str:
    # å…¨è§’/åŠè§’ãƒ»æ¿ç‚¹ãªã©ã‚’çµ±ä¸€ã—ã¦ã‹ã‚‰å°æ–‡å­—åŒ–
    s = unicodedata.normalize("NFKC", s or "")
    return s.lower()

def any_hit(text_lc: str, words: list[str]) -> bool:
    if not words:
        return False
    for w in set(words):
        w = norm(w)
        if w and w in text_lc:
            return True
    return False

def none_hit(text_lc: str, words: list[str]) -> bool:
    if not words:
        return True
    for w in set(words):
        w = norm(w)
        if w and w in text_lc:
            return False
    return True

# ==== feeds.txt ã®èª­ã¿è¾¼ã¿ ====
# 1è¡Œ:  URL | <å«ã‚ã‚‹èªspec> | <é™¤å¤–èªspec>
# <spec>:
#   "= èª èª ..."  â†’ ä¸Šæ›¸ãï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ç„¡è¦–ï¼‰
#   "+ èª èª ..."  â†’ è¿½åŠ ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¶³ã™ï¼‰
#   "& èª èª ..."  â†’ ä¸¡æ–¹ï¼ˆGLOBAL_INCLUDE ã¨å½“è©²èªã®ä¸¡æ–¹ã«ãƒ’ãƒƒãƒˆã§æ¡ç”¨ï¼‰ â† è¿½åŠ 
#   "èª èª ..."    â†’ è¿½åŠ ï¼ˆæ¥é ­è¾ãªã—ã¯ + ã¨åŒã˜ï¼‰
#   "ALL" / "*" / "ALL!" â†’ ã“ã®RSSã¯å…¨ä»¶é€šã™ï¼ˆ2åˆ—ç›®ã«æ›¸ãï¼‰ã€‚ãŸã ã—3åˆ—ç›®ã®é™¤å¤–èªã¯æœ‰åŠ¹
def read_feeds_with_rules(path: Path):
    if not path.exists():
        return []
    text = None
    for enc in ("utf-8-sig", "cp932"):
        try:
            text = path.read_text(encoding=enc)
            break
        except UnicodeDecodeError:
            continue
    if text is None:
        text = path.read_text(encoding="utf-8", errors="ignore")

    def parse_spec(spec: str):
        spec = spec.strip()
        mode = "add"  # add / override / both
        if not spec:
            return mode, []
        head = spec[:1]
        if head == "=":
            mode = "override"; spec = spec[1:].strip()
        elif head in {"+", "-"}:
            mode = "add"; spec = spec[1:].strip()
        elif head == "&":  # â† æœ€å°å·®åˆ†: both ãƒ¢ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            mode = "both"; spec = spec[1:].strip()
        words = [w for w in spec.split() if w]
        return mode, words

    feeds = []
    for line in text.splitlines():
        s = line.strip()
        if not s or s.startswith("#"):
            continue
        parts = [p.strip() for p in s.split("|")]
        url = parts[0]
        inc_spec = parts[1] if len(parts) >= 2 else ""
        exc_spec = parts[2] if len(parts) >= 3 else ""

        inc_mode, inc_words = parse_spec(inc_spec)
        exc_mode, exc_words = parse_spec(exc_spec)

        inc_upper = inc_spec.strip().upper()
        pass_all = inc_upper in ("ALL", "*", "ALL!")

        # ALLã§ã‚‚ exc_words ã¯ä¿æŒï¼ˆinc ã¯ç„¡è¦–ï¼‰
        feeds.append({
            "url": url,
            "pass_all": pass_all,
            "inc_mode": "override" if pass_all else inc_mode,
            "inc_words": [] if pass_all else inc_words,
            "exc_mode": exc_mode,
            "exc_words": exc_words,
        })
    return feeds

# ==== ã‚¢ã‚¤ãƒ†ãƒ æŠ½å‡º â†’ HTML ====
def fetch_items(feed_rules: list[dict]):
    items = []
    total_entries = 0

    if not feed_rules:
        feed_rules = [{"url": u, "pass_all": False,
                       "inc_mode":"add","inc_words":[],
                       "exc_mode":"add","exc_words":[]} for u in DEFAULT_FEEDS]
        print(f"[info] feeds.txt ãŒç„¡ã„/ç©º â†’ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ{len(feed_rules)}æœ¬ã§å®Ÿè¡Œ")

    for fr in feed_rules:
        url = fr["url"]
        pass_all = fr.get("pass_all", False)

        # inc/exc æ±ºå®šã€‚ALLæ™‚ã¯ inc ã‚’ä½¿ã‚ãšã€exc ã¯æ´»ã‹ã™
        if pass_all:
            inc = []
            exc = fr["exc_words"] if fr["exc_mode"] == "override" else (GLOBAL_EXCLUDE + fr["exc_words"])
        else:
            inc_mode = fr.get("inc_mode", "add")
            if inc_mode == "override":
                inc = fr["inc_words"]
            elif inc_mode == "both":
                # ä¸¡æ–¹ãƒ’ãƒƒãƒˆã¯ accept åˆ¤å®šã§å‡¦ç†ã™ã‚‹ã®ã§ã€ã“ã“ã§ã¯ãƒ•ã‚£ãƒ¼ãƒ‰å›ºæœ‰èªã®ã¿ã‚’ä¿æŒ
                inc = fr["inc_words"]
            else:
                inc = GLOBAL_INCLUDE + fr["inc_words"]
            exc = fr["exc_words"] if fr["exc_mode"] == "override" else (GLOBAL_EXCLUDE + fr["exc_words"])

        print(f"[fetch] {url} {'(ALL)' if pass_all else ''}")
        start = time.time()
        try:
            d = feedparser.parse(url)
        except Exception as e:
            print(f"[error] {url} â†’ {e}")
            continue
        print(f"[ok] {url} entries={len(d.entries)} {time.time()-start:.1f}s")

        for e in d.entries:
            total_entries += 1
            title = (e.get("title") or "").strip()
            link  = e.get("link") or ""
            # RSSã®æœ¬æ–‡/è¦ç´„ï¼ˆã‚ã‚Œã°ï¼‰
            body = ""
            if e.get("content") and isinstance(e["content"], list) and e["content"]:
                body = clean_html(e["content"][0].get("value") or "")
            if not body:
                body = clean_html(e.get("summary") or e.get("description") or "")

            hay = f"{title}\n{body}"
            hay_lc = norm(hay)

            # å—ç†åˆ¤å®šï¼š
            #  - ALL: é™¤å¤–èªã ã‘ãƒã‚§ãƒƒã‚¯
            #  - both: GLOBAL_INCLUDE ã¨ inc(ãƒ•ã‚£ãƒ¼ãƒ‰å›ºæœ‰èª) ã®ä¸¡æ–¹ã«ãƒ’ãƒƒãƒˆ ã‹ã¤ éé™¤å¤–
            #  - add/override: å¾“æ¥é€šã‚Š (inc ã«ãƒ’ãƒƒãƒˆ) ã‹ã¤ éé™¤å¤–
            if pass_all:
                accept = none_hit(hay_lc, exc)
            else:
                if fr.get("inc_mode") == "both":
                    accept = any_hit(hay_lc, GLOBAL_INCLUDE) and any_hit(hay_lc, inc) and none_hit(hay_lc, exc)
                else:
                    accept = any_hit(hay_lc, inc) and none_hit(hay_lc, exc)

            if not accept:
                continue

            # æ—¥ä»˜
            pub = None
            for key in ("published_parsed", "updated_parsed"):
                if e.get(key):
                    pub = to_iso(e.get(key))
                    break
            if not pub:
                pub = to_iso(None)

            print("APPEND:", title, link)

            items.append({
                "title": title,
                "url": link,
                "source": host_of(link),
                "published": pub,
            })

    items.sort(key=lambda x: x["published"], reverse=True)
    print(f"[sum] total_entries={total_entries}, extracted={len(items)}")
    return items[:MAX_ITEMS]

def build_html(items):
    css = """
    body{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,"Noto Sans JP",sans-serif;line-height:1.6;margin:20px;}
    header{margin-bottom:16px}
    h1{font-size:1.45rem;margin:0}
    .desc{color:#555;margin:4px 0 12px}
    .date{margin-top:22px;font-weight:700}
    .item{border:1px solid #eee;border-radius:12px;padding:10px 12px;margin:10px 0}
    .item h3{margin:0 0 6px;font-size:1.02rem}
    .meta{font-size:.85rem;color:#666}
    a{color:#0a58ca;text-decoration:none}
    a:hover{text-decoration:underline}
    footer{color:#777;font-size:.85rem;margin-top:24px}
    """

    # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°
    groups = {}
    for it in items:
        day = iso_to_ymd_jst(it["published"]) or "æ—¥ä»˜ä¸æ˜"
        groups.setdefault(day, []).append(it)

    # æœ€çµ‚æ›´æ–°è¡¨ç¤ºç”¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã§è¡¨ç¤ºï¼‰
    now_str = datetime.now().astimezone().strftime("%Y-%m-%d %H:%M")

    parts = [
        "<!DOCTYPE html>",
        "<html lang=\"ja\">",
        "<head>",
        "<meta charset=\"utf-8\">",
        f"<title>{html.escape(SITE_TITLE_TEXT)}</title>",  # ã‚¿ãƒ–ç”¨ï¼ˆæ”¹è¡Œãªã—ï¼‰
        "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">",
        f"<meta name=\"description\" content=\"{html.escape(SITE_DESC)}\">",
        f"<style>{css}</style>",
        "</head>",
        "<body>",
        "<header>",
        f"<h1>{SITE_TITLE_HTML}</h1>",                      # è¦‹å‡ºã—ç”¨ï¼ˆæ”¹è¡Œã‚ã‚ŠOKï¼‰
        f'<div class="desc">{SITE_DESC}</div>',
        "</header>",
    ]

    for day in sorted(groups.keys(), reverse=True):
        parts.append(f"<div class=\"date\">ğŸ“… {day}</div>")
        for it in groups[day]:
            title = html.escape(it["title"] or "(ç„¡é¡Œ)")
            url   = html.escape(it["url"] or "#")
            src   = html.escape(it["source"] or "")
            parts.append(
                f"<div class=\"item\">"
                f"<h3><a href=\"{url}\" target=\"_blank\" rel=\"noopener\">{title}</a></h3>"
                f"<div class=\"meta\">å‡ºå…¸: {src}</div>"
                f"</div>"
            )

    parts += [
        f'<footer>æœ€çµ‚æ›´æ–°: {now_str}ï¼ˆJSTï¼‰ï½œ <a href="https://www.greo-jp.com/" target="_blank">Operated by GREO</a></footer>',
        "</body></html>",
    ]

    # HTMLæ–‡å­—åˆ—ã‚’ä¸€åº¦ä½œã£ã¦ã€index ã¨ archive ã®ä¸¡æ–¹ã¸æ›¸ãå‡ºã—
    html_text = "\n".join(parts)

    # æœ€æ–°
    out = SITE_DIR / "index.html"
    out.write_text(html_text, encoding="utf-8")

    # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼ˆå‰Šé™¤ãªã—ã§è²¯ã‚ã‚‹ï¼‰
    archive_dir = SITE_DIR / "archive"
    archive_dir.mkdir(parents=True, exist_ok=True)
    archive_name = datetime.now().strftime("%Y-%m-%d") + ".html"
    (archive_dir / archive_name).write_text(html_text, encoding="utf-8")

    return out

def main():
    feeds_path = CONFIG_DIR / "feeds.txt"
    feed_rules = read_feeds_with_rules(feeds_path)
    items = fetch_items(feed_rules)
    out = build_html(items)
    print(f"ç”Ÿæˆ: {out}ï¼ˆ{len(items)}ä»¶ï¼‰")

if __name__ == "__main__":
    main()
